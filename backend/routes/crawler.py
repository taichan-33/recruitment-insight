from flask import Blueprint, jsonify, request
import threading
from shared_state import crawler_status
from scheduler import get_schedules, add_schedule, remove_schedule

crawler_bp = Blueprint("crawler", __name__)


@crawler_bp.route("/api/crawl", methods=["POST"])
def run_crawl():
    """ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’å®Ÿè¡Œã™ã‚‹"""
    if crawler_status["is_running"]:
        return (
            jsonify({"status": "error", "message": "ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã¯æ—¢ã«å®Ÿè¡Œä¸­ã§ã™"}),
            400,
        )

    data = request.get_json() or {}
    prefecture = data.get("prefecture", "åŒ—æµ·é“")
    max_pages = data.get("max_pages", 10)
    force = data.get("force", False)  # å¼·åˆ¶åé›†ãƒ¢ãƒ¼ãƒ‰

    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’å®Ÿè¡Œ
    def run_crawler_thread():
        crawler_status["is_running"] = True
        crawler_status["last_error"] = None

        try:
            from crawler import run_crawler

            run_crawler(
                prefecture=prefecture, max_pages=max_pages, headless=False, force=force
            )
            crawler_status["last_result"] = {
                "success": True,
                "prefecture": prefecture,
                "max_pages": max_pages,
                "force": force,
            }
        except Exception as e:
            crawler_status["last_error"] = str(e)
            crawler_status["last_result"] = {"success": False, "error": str(e)}
        finally:
            crawler_status["is_running"] = False

    thread = threading.Thread(target=run_crawler_thread)
    thread.start()

    return (
        jsonify(
            {
                "status": "started",
                "message": f"ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’é–‹å§‹ã—ã¾ã—ãŸ: {prefecture}, {max_pages}ãƒšãƒ¼ã‚¸, {'å¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰' if force else 'é€šå¸¸ãƒ¢ãƒ¼ãƒ‰'}",
            }
        ),
        202,
    )


@crawler_bp.route("/api/crawl/indeed", methods=["POST"])
def run_crawl_indeed():
    """Indeedã‹ã‚‰æ±‚äººã‚’åé›†"""
    if crawler_status["is_running"]:
        return (
            jsonify({"status": "error", "message": "ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã¯æ—¢ã«å®Ÿè¡Œä¸­ã§ã™"}),
            400,
        )

    data = request.get_json() or {}
    keyword = data.get("keyword", "")
    location = data.get("location", "æ±äº¬éƒ½")
    max_pages = data.get("max_pages", 3)

    def run_indeed_thread():
        crawler_status["is_running"] = True
        crawler_status["last_error"] = None

        try:
            print("ğŸš€ Indeedã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹...")
            from indeed_crawler import run_indeed_crawler

            print(f"ğŸ“¦ indeed_crawler ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")

            result = run_indeed_crawler(
                keyword=keyword, location=location, max_pages=max_pages, headless=False
            )
            crawler_status["last_result"] = result
            print(f"âœ… ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼å®Œäº†: {result}")
        except Exception as e:
            import traceback

            print(f"âŒ Indeedã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            crawler_status["last_error"] = str(e)
            crawler_status["last_result"] = {"success": False, "error": str(e)}
        finally:
            crawler_status["is_running"] = False

    thread = threading.Thread(target=run_indeed_thread)
    thread.start()

    return (
        jsonify(
            {
                "status": "started",
                "message": f"Indeedæ¤œç´¢ã‚’é–‹å§‹: {keyword or 'å…¨ã¦'} @ {location}",
            }
        ),
        202,
    )


@crawler_bp.route("/api/crawl/region", methods=["POST"])
def run_crawl_region():
    """è¤‡æ•°éƒ½é“åºœçœŒã‚’ä¸€æ‹¬åé›†"""
    if crawler_status["is_running"]:
        return (
            jsonify({"status": "error", "message": "ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã¯æ—¢ã«å®Ÿè¡Œä¸­ã§ã™"}),
            400,
        )

    data = request.get_json() or {}
    region = data.get("region", "kanto")
    max_pages = data.get("max_pages", 3)
    force = data.get("force", False)
    keyword = data.get("keyword", "")

    from crawler import get_prefectures_by_region

    prefectures = get_prefectures_by_region(region)

    def run_region_crawler():
        crawler_status["is_running"] = True
        crawler_status["last_error"] = None

        try:
            from crawler import run_crawler

            total = 0
            for i, pref in enumerate(prefectures):
                print(f"\nğŸŒ [{i+1}/{len(prefectures)}] {pref}ã‚’åé›†ä¸­...")
                run_crawler(
                    prefecture=pref,
                    max_pages=max_pages,
                    headless=True,
                    force=force,
                    keyword=keyword,
                )
                total += 1

            crawler_status["last_result"] = {
                "success": True,
                "region": region,
                "prefectures_count": total,
            }
        except Exception as e:
            crawler_status["last_error"] = str(e)
            crawler_status["last_result"] = {"success": False, "error": str(e)}
        finally:
            crawler_status["is_running"] = False

    thread = threading.Thread(target=run_region_crawler)
    thread.start()

    return (
        jsonify(
            {
                "status": "started",
                "message": f"{len(prefectures)}éƒ½é“åºœçœŒã®åé›†ã‚’é–‹å§‹ã—ã¾ã—ãŸ",
                "prefectures": prefectures,
            }
        ),
        202,
    )


@crawler_bp.route("/api/crawl/status")
def get_crawl_status():
    """ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã®å®Ÿè¡ŒçŠ¶æ…‹ã‚’å–å¾—"""
    return jsonify(
        {
            "is_running": crawler_status["is_running"],
            "last_result": crawler_status["last_result"],
            "last_error": crawler_status["last_error"],
        }
    )


@crawler_bp.route("/api/schedules")
def get_schedules_api():
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    return jsonify(get_schedules())


@crawler_bp.route("/api/schedules", methods=["POST"])
def add_schedule_api():
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¿½åŠ """
    data = request.get_json() or {}
    result = add_schedule(
        name=data.get("name", "default"),
        prefecture=data.get("prefecture", "æ±äº¬éƒ½"),
        interval_hours=data.get("interval_hours", 24),
        max_pages=data.get("max_pages", 10),
        keyword=data.get("keyword", ""),
        force=data.get("force", False),
    )
    return jsonify(result)


@crawler_bp.route("/api/schedules/<name>", methods=["DELETE"])
def remove_schedule_api(name):
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å‰Šé™¤"""
    result = remove_schedule(name)
    return jsonify(result)
