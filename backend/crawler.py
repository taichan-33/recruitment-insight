# crawler.py
import re
import time
import sqlite3
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from database import save_job_if_not_duplicate, get_connection, init_db

# ==========================================
# 1. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° (TDDæ¸ˆã¿)
# ==========================================


def clean_money(text):
    """é‡‘é¡æ–‡å­—åˆ—ã‚’æ•°å€¤ã«å¤‰æ›ã™ã‚‹"""
    if not text:
        return 0
    clean_text = re.sub(r"[^\d]", "", str(text))
    if not clean_text:
        return 0
    return int(clean_text)


# æ¥­ç•Œåˆ†é¡ã®ãŸã‚ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸ï¼ˆå„ªå…ˆé †ä½ã®é«˜ã„é †ï¼‰
INDUSTRY_KEYWORDS = {
    "è£½é€ ãƒ»å»ºè¨­": [
        "å·¥å ´",
        "è£½é€ ",
        "å»ºè¨­",
        "æ–½å·¥ç®¡ç†",
        "æ–½å·¥",
        "é›»æ°—å·¥äº‹",
        "è¨­å‚™",
        "æ©Ÿæ¢°",
        "æº¶æ¥",
        "çµ„ç«‹",
        "æ¤œå“",
        "å€‰åº«",
        "ç‰©æµ",
        "é…é€",
        "ç¾å ´",
        "ä½œæ¥­å“¡",
    ],
    "ITãƒ»ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢": [
        "ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢",
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒ",
        "SE",
        "é–‹ç™º",
        "IT",
        "Web",
        "ã‚·ã‚¹ãƒ†ãƒ ",
        "ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢",
        "ã‚¤ãƒ³ãƒ•ãƒ©",
        "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
        "ãƒ‡ãƒ¼ã‚¿",
        "AI",
        "æ©Ÿæ¢°å­¦ç¿’",
    ],
    "åŒ»ç™‚ãƒ»ä»‹è­·": [
        "çœ‹è­·",
        "ä»‹è­·",
        "åŒ»ç™‚",
        "ç—…é™¢",
        "ã‚¯ãƒªãƒ‹ãƒƒã‚¯",
        "ç¦ç¥‰",
        "ä¿è‚²",
        "ã‚±ã‚¢",
        "ãƒ˜ãƒ«ãƒ‘ãƒ¼",
        "ãƒªãƒãƒ“ãƒª",
        "è–¬å‰¤",
        "æ¤œæŸ»æŠ€å¸«",
        "æ­¯ç§‘",
    ],
    "ã‚µãƒ¼ãƒ“ã‚¹ãƒ»è²©å£²": [
        "è²©å£²",
        "æ¥å®¢",
        "åº—èˆ—",
        "ãƒ¬ã‚¸",
        "é£²é£Ÿ",
        "èª¿ç†",
        "ãƒ›ãƒ†ãƒ«",
        "ã‚µãƒ¼ãƒ“ã‚¹",
        "æ¸…æƒ",
        "ç¾å®¹",
        "ç†å®¹",
    ],
    "å–¶æ¥­ãƒ»äº‹å‹™": [
        "å–¶æ¥­",
        "äº‹å‹™",
        "çµŒç†",
        "ç·å‹™",
        "äººäº‹",
        "ç§˜æ›¸",
        "å—ä»˜",
        "ã‚³ãƒ¼ãƒ«ã‚»ãƒ³ã‚¿ãƒ¼",
        "ã‚«ã‚¹ã‚¿ãƒãƒ¼",
        "ã‚µãƒãƒ¼ãƒˆ",
        "ç®¡ç†",
    ],
}


def classify_industry(title):
    """
    è·ç¨®ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ¥­ç•Œã‚’æ¨å®šã™ã‚‹

    Args:
        title: è·ç¨®ã‚¿ã‚¤ãƒˆãƒ«

    Returns:
        æ¥­ç•Œåï¼ˆITãƒ»ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€åŒ»ç™‚ãƒ»ä»‹è­·ã€å–¶æ¥­ãƒ»äº‹å‹™ã€ã‚µãƒ¼ãƒ“ã‚¹ãƒ»è²©å£²ã€è£½é€ ãƒ»å»ºè¨­ã€ãã®ä»–ï¼‰
    """
    if not title:
        return "ãã®ä»–"

    title_upper = title.upper()

    for industry, keywords in INDUSTRY_KEYWORDS.items():
        for keyword in keywords:
            if keyword.upper() in title_upper:
                return industry

    return "ãã®ä»–"


def parse_job_html(element):
    """
    æœ¬ç•ªç”¨: ãƒãƒ­ãƒ¼ãƒ¯ãƒ¼ã‚¯ã®æ±‚äººã‚«ãƒ¼ãƒ‰(table.kyujin)ã‚’å—ã‘å–ã‚Šè¾æ›¸ã‚’è¿”ã™
    å®Ÿéš›ã®ãƒãƒ­ãƒ¼ãƒ¯ãƒ¼ã‚¯HTMLæ§‹é€ ã«å¯¾å¿œï¼ˆãƒã‚¹ãƒˆã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰
    """
    try:
        # ã‚¿ã‚¤ãƒˆãƒ«: kyujin_headã‚¯ãƒ©ã‚¹ã®è¡Œã‹ã‚‰å–å¾—
        title = ""
        head = element.find("tr", class_="kyujin_head")
        if head:
            # æœ€åˆã®tdå†…ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒªãƒ³ã‚¯å†…ã®å ´åˆã‚‚ã‚ã‚‹ï¼‰
            link = head.find("a")
            if link:
                title = link.get_text(strip=True)
            else:
                first_td = head.find("td")
                if first_td:
                    title = first_td.get_text(strip=True)

        # kyujin_bodyå†…ã®ãƒã‚¹ãƒˆã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
        body = element.find("tr", class_="kyujin_body")

        company = ""
        location = ""
        wage_text = ""

        if body:
            # ãƒã‚¹ãƒˆã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«ã®å…¨è¡Œã‚’æ¤œç´¢
            inner_rows = body.find_all("tr", class_="border_new")

            for row in inner_rows:
                tds = row.find_all("td")
                if len(tds) >= 2:
                    label = tds[0].get_text(strip=True)
                    value = tds[1].get_text(strip=True)

                    if "äº‹æ¥­æ‰€å" in label:
                        company = value
                    elif "å°±æ¥­å ´æ‰€" in label:
                        location = value
                    elif "è³ƒé‡‘" in label:
                        wage_text = value
                    elif "ä»•äº‹ã®å†…å®¹" in label and not title:
                        # ã‚¿ã‚¤ãƒˆãƒ«ãŒå–ã‚Œãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        title = value[:100] if value else ""

        # çµ¦ä¸æƒ…å ±ã®æŠ½å‡º
        full_text = element.text

        # è³ƒé‡‘å½¢æ…‹ã®åˆ¤å®š
        wage_type = "unknown"
        if "æ™‚çµ¦" in full_text:
            wage_type = "hourly"
        elif "æœˆçµ¦" in full_text:
            wage_type = "monthly"
        elif "æ—¥çµ¦" in full_text:
            wage_type = "daily"
        elif "å¹´ä¿¸" in full_text:
            wage_type = "annual"

        # é‡‘é¡ã®æŠ½å‡ºï¼ˆwage_textã‹ã‚‰å„ªå…ˆã€ãªã‘ã‚Œã°full_textã‹ã‚‰ï¼‰
        wages = re.findall(r"([\d,]+)å††", wage_text if wage_text else "")
        if not wages:
            wages = re.findall(r"([\d,]+)å††", full_text)

        wage_min = 0
        wage_max = 0
        if len(wages) >= 1:
            wage_min = clean_money(wages[0])
            wage_max = clean_money(wages[1]) if len(wages) >= 2 else wage_min

        # é‡‘é¡ã‹ã‚‰è³ƒé‡‘å½¢æ…‹ã‚’æ¨æ¸¬
        if wage_type == "unknown" and wage_min > 0:
            if wage_min < 10000:
                wage_type = "hourly"
            elif wage_min >= 100000:
                wage_type = "monthly"

        return {
            "title": title[:100] if title else "",
            "company": company,
            "location": location,
            "wage_min": wage_min,
            "wage_max": wage_max,
            "wage_type": wage_type,
            "url": "",
        }

    except Exception as e:
        print(f"  âš ï¸ ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        return None


# ==========================================
# 2. ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼å®Ÿè¡Œå‡¦ç†ï¼ˆè‡ªå‹•åŒ–ç‰ˆï¼‰
# ==========================================

# éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ã®è¾æ›¸
PREFECTURE_CODES = {
    "åŒ—æµ·é“": "01",
    "é’æ£®çœŒ": "02",
    "å²©æ‰‹çœŒ": "03",
    "å®®åŸçœŒ": "04",
    "ç§‹ç”°çœŒ": "05",
    "å±±å½¢çœŒ": "06",
    "ç¦å³¶çœŒ": "07",
    "èŒ¨åŸçœŒ": "08",
    "æ ƒæœ¨çœŒ": "09",
    "ç¾¤é¦¬çœŒ": "10",
    "åŸ¼ç‰çœŒ": "11",
    "åƒè‘‰çœŒ": "12",
    "æ±äº¬éƒ½": "13",
    "ç¥å¥ˆå·çœŒ": "14",
    "æ–°æ½ŸçœŒ": "15",
    "å¯Œå±±çœŒ": "16",
    "çŸ³å·çœŒ": "17",
    "ç¦äº•çœŒ": "18",
    "å±±æ¢¨çœŒ": "19",
    "é•·é‡çœŒ": "20",
    "å²é˜œçœŒ": "21",
    "é™å²¡çœŒ": "22",
    "æ„›çŸ¥çœŒ": "23",
    "ä¸‰é‡çœŒ": "24",
    "æ»‹è³€çœŒ": "25",
    "äº¬éƒ½åºœ": "26",
    "å¤§é˜ªåºœ": "27",
    "å…µåº«çœŒ": "28",
    "å¥ˆè‰¯çœŒ": "29",
    "å’Œæ­Œå±±çœŒ": "30",
    "é³¥å–çœŒ": "31",
    "å³¶æ ¹çœŒ": "32",
    "å²¡å±±çœŒ": "33",
    "åºƒå³¶çœŒ": "34",
    "å±±å£çœŒ": "35",
    "å¾³å³¶çœŒ": "36",
    "é¦™å·çœŒ": "37",
    "æ„›åª›çœŒ": "38",
    "é«˜çŸ¥çœŒ": "39",
    "ç¦å²¡çœŒ": "40",
    "ä½è³€çœŒ": "41",
    "é•·å´çœŒ": "42",
    "ç†Šæœ¬çœŒ": "43",
    "å¤§åˆ†çœŒ": "44",
    "å®®å´çœŒ": "45",
    "é¹¿å…å³¶çœŒ": "46",
    "æ²–ç¸„çœŒ": "47",
}

# åœ°åŸŸåˆ¥éƒ½é“åºœçœŒãƒãƒƒãƒ”ãƒ³ã‚°
REGION_PREFECTURES = {
    "hokkaido_tohoku": [
        "åŒ—æµ·é“",
        "é’æ£®çœŒ",
        "å²©æ‰‹çœŒ",
        "å®®åŸçœŒ",
        "ç§‹ç”°çœŒ",
        "å±±å½¢çœŒ",
        "ç¦å³¶çœŒ",
    ],
    "kanto": ["èŒ¨åŸçœŒ", "æ ƒæœ¨çœŒ", "ç¾¤é¦¬çœŒ", "åŸ¼ç‰çœŒ", "åƒè‘‰çœŒ", "æ±äº¬éƒ½", "ç¥å¥ˆå·çœŒ"],
    "chubu": [
        "æ–°æ½ŸçœŒ",
        "å¯Œå±±çœŒ",
        "çŸ³å·çœŒ",
        "ç¦äº•çœŒ",
        "å±±æ¢¨çœŒ",
        "é•·é‡çœŒ",
        "å²é˜œçœŒ",
        "é™å²¡çœŒ",
        "æ„›çŸ¥çœŒ",
    ],
    "kansai": ["ä¸‰é‡çœŒ", "æ»‹è³€çœŒ", "äº¬éƒ½åºœ", "å¤§é˜ªåºœ", "å…µåº«çœŒ", "å¥ˆè‰¯çœŒ", "å’Œæ­Œå±±çœŒ"],
    "chugoku": ["é³¥å–çœŒ", "å³¶æ ¹çœŒ", "å²¡å±±çœŒ", "åºƒå³¶çœŒ", "å±±å£çœŒ"],
    "shikoku": ["å¾³å³¶çœŒ", "é¦™å·çœŒ", "æ„›åª›çœŒ", "é«˜çŸ¥çœŒ"],
    "kyushu": [
        "ç¦å²¡çœŒ",
        "ä½è³€çœŒ",
        "é•·å´çœŒ",
        "ç†Šæœ¬çœŒ",
        "å¤§åˆ†çœŒ",
        "å®®å´çœŒ",
        "é¹¿å…å³¶çœŒ",
        "æ²–ç¸„çœŒ",
    ],
}


def get_prefectures_by_region(region):
    """
    åœ°åŸŸåã‹ã‚‰éƒ½é“åºœçœŒãƒªã‚¹ãƒˆã‚’å–å¾—

    Args:
        region: åœ°åŸŸå ('all', 'kanto', 'kansai' ãªã©)

    Returns:
        éƒ½é“åºœçœŒåã®ãƒªã‚¹ãƒˆ
    """
    if region == "all":
        return list(PREFECTURE_CODES.keys())

    return REGION_PREFECTURES.get(region, [])


def run_crawler(
    prefecture="åŒ—æµ·é“", max_pages=3, headless=False, force=False, keyword=""
):
    """
    ãƒãƒ­ãƒ¼ãƒ¯ãƒ¼ã‚¯æ±‚äººã‚’è‡ªå‹•åé›†ã™ã‚‹

    Args:
        prefecture: æ¤œç´¢ã™ã‚‹éƒ½é“åºœçœŒåï¼ˆä¾‹: "åŒ—æµ·é“", "å¤§é˜ªåºœ"ï¼‰
        max_pages: å–å¾—ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°ï¼ˆ1ãƒšãƒ¼ã‚¸50ä»¶ï¼‰
        headless: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹ã‹
        force: Trueã®å ´åˆã€é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦å¼·åˆ¶ä¿å­˜
    """
    mode = "å¼·åˆ¶" if force else "é€šå¸¸"
    print(
        f"ğŸš€ ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’èµ·å‹•ä¸­... (å¯¾è±¡: {prefecture}, æœ€å¤§{max_pages}ãƒšãƒ¼ã‚¸, {mode}ãƒ¢ãƒ¼ãƒ‰)"
    )

    options = Options()
    if headless:
        options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument(
        "user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )

    driver = webdriver.Chrome(
        service=ChromeService(ChromeDriverManager().install()), options=options
    )
    driver.implicitly_wait(5)
    wait = WebDriverWait(driver, 30)

    try:
        # ãƒãƒ­ãƒ¼ãƒ¯ãƒ¼ã‚¯æ±‚äººæ¤œç´¢ãƒšãƒ¼ã‚¸ã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
        print("ğŸ“ æ±‚äººæ¤œç´¢ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
        driver.get(
            "https://www.hellowork.mhlw.go.jp/kensaku/GECA110010.do?action=initDisp&screenId=GECA110010"
        )

        # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å®Œäº†ã‚’å¾…æ©Ÿ
        WebDriverWait(driver, 30).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        time.sleep(3)

        # éƒ½é“åºœçœŒã‚’é¸æŠï¼ˆSELECTãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ï¼‰
        print(f"ğŸ“ éƒ½é“åºœçœŒã‚’é¸æŠä¸­: {prefecture}")
        try:
            pref_code = PREFECTURE_CODES.get(prefecture, "01")
            dropdown = wait.until(
                EC.visibility_of_element_located((By.ID, "ID_tDFK1CmbBox"))
            )
            select = Select(dropdown)
            select.select_by_value(pref_code)
            print(f"  âœ… {prefecture}ã‚’é¸æŠã—ã¾ã—ãŸ")
            time.sleep(2)
        except Exception as e:
            print(f"  âš ï¸ éƒ½é“åºœçœŒé¸æŠã§ã‚¨ãƒ©ãƒ¼: {e}")
            print("  â†’ å…¨å›½æ¤œç´¢ã§ç¶šè¡Œã—ã¾ã™")

        # æ¤œç´¢ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼ˆJavaScriptçµŒç”±ï¼‰
        print("ğŸ“ æ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")
        try:
            search_button = wait.until(
                EC.visibility_of_element_located((By.ID, "ID_searchBtn"))
            )
            driver.execute_script("arguments[0].scrollIntoView(true);", search_button)
            time.sleep(1)
            driver.execute_script("arguments[0].click();", search_button)
            time.sleep(5)
            print("  âœ… æ¤œç´¢å®Ÿè¡Œå®Œäº†")
        except Exception as e:
            print(f"  âŒ æ¤œç´¢ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼: {e}")
            raise

        conn = get_connection()
        total_count = 0

        # ãƒšãƒ¼ã‚¸ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿åé›†
        for page in range(1, max_pages + 1):
            print(f"\nğŸ“¥ ãƒšãƒ¼ã‚¸ {page}/{max_pages} ã‚’è§£æä¸­...")

            html = driver.page_source
            soup = BeautifulSoup(html, "html.parser")

            # æ±‚äººãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
            job_rows = soup.select("table.kyujin")

            if not job_rows:
                print("  âš ï¸ ã“ã®ãƒšãƒ¼ã‚¸ã«æ±‚äººãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                break

            page_count = 0
            skip_count = 0
            for row in job_rows:
                data = parse_job_html(row)
                if data:
                    # æ¥­ç•Œã‚’è‡ªå‹•åˆ†é¡
                    industry = classify_industry(data["title"])

                    job_tuple = (
                        data["title"],
                        data["wage_min"],
                        data["wage_max"],
                        data["wage_type"],
                        data["company"],
                        data["location"],
                        data["url"],
                        industry,  # æ¥­ç•Œåˆ†é¡ã‚’è¿½åŠ 
                    )

                    # å¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if force:
                        from database import save_job_to_db

                        save_job_to_db(conn, job_tuple)
                        page_count += 1
                        print(
                            f"  - [{data['wage_type']}][{industry}]: {data['title'][:25]}... ({data['wage_min']}å††)"
                        )
                    elif save_job_if_not_duplicate(conn, job_tuple):
                        page_count += 1
                        print(
                            f"  - [{data['wage_type']}][{industry}]: {data['title'][:25]}... ({data['wage_min']}å††)"
                        )
                    else:
                        skip_count += 1

            total_count += page_count
            if force:
                print(f"  âœ… {page_count}ä»¶ã‚’ä¿å­˜ (å¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰)")
            else:
                print(f"  âœ… {page_count}ä»¶ã‚’ä¿å­˜ ({skip_count}ä»¶ã¯é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—)")

            # æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸
            if page < max_pages:
                try:
                    # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã€Œæ¬¡ã¸ã€ãƒœã‚¿ãƒ³ã‚’æ¢ã™
                    next_button = None
                    for selector in [
                        "//input[@value='æ¬¡ã¸']",
                        "//button[contains(text(), 'æ¬¡ã¸')]",
                        "//a[contains(text(), 'æ¬¡')]",
                        "//input[contains(@value, 'æ¬¡')]",
                    ]:
                        try:
                            next_button = driver.find_element(By.XPATH, selector)
                            break
                        except:
                            continue

                    if next_button and next_button.is_enabled():
                        driver.execute_script("arguments[0].click();", next_button)
                        time.sleep(3)
                    else:
                        print("  â†’ æœ€å¾Œã®ãƒšãƒ¼ã‚¸ã«åˆ°é”ã—ã¾ã—ãŸ")
                        break
                except Exception as e:
                    print(f"  â†’ æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“: {e}")
                    break

        conn.close()
        print(f"\nğŸ‰ å®Œäº†ï¼ åˆè¨ˆ {total_count} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ jobs.db ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback

        traceback.print_exc()

    finally:
        time.sleep(2)
        driver.quit()


if __name__ == "__main__":
    import sys

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰éƒ½é“åºœçœŒã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: åŒ—æµ·é“ï¼‰
    prefecture = sys.argv[1] if len(sys.argv) > 1 else "åŒ—æµ·é“"
    max_pages = int(sys.argv[2]) if len(sys.argv) > 2 else 3

    run_crawler(prefecture=prefecture, max_pages=max_pages)
