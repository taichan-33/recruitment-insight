# backend/indeed_crawler.py
"""
Indeedæ±‚äººã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»åœ°åŸŸã§æ±‚äººã‚’æ¤œç´¢ãƒ»åé›†
undetected-chromedriverã§CAPTCHAå›é¿
"""
import re
import time
import sqlite3
from bs4 import BeautifulSoup
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from database import get_connection, init_db
from crawler import classify_industry, clean_money


def parse_indeed_job(card):
    """
    Indeedæ±‚äººã‚«ãƒ¼ãƒ‰ã‚’ãƒ‘ãƒ¼ã‚¹
    å®Ÿéš›ã®HTMLæ§‹é€ :
    - çµ¦ä¸: li.salary-snippet-containerå†…ã®ãƒ†ã‚­ã‚¹ãƒˆ
    - é›‡ç”¨å½¢æ…‹: li[data-testid="attribute_snippet_testid"]ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆçµ¦ä¸ä»¥å¤–ï¼‰
    """
    try:
        # ã‚¿ã‚¤ãƒˆãƒ«
        title_elem = card.find("h2", class_="jobTitle")
        title = title_elem.get_text(strip=True) if title_elem else ""

        # URL
        url = ""
        if title_elem:
            link_elem = title_elem.find("a")
            if link_elem:
                href = link_elem.get("href")
                if href:
                    if href.startswith("/"):
                        url = f"https://jp.indeed.com{href}"
                    else:
                        url = href

        # ã€Œæ–°ç€ã€ã‚¿ã‚°ã‚’é™¤å»
        if title.startswith("æ–°ç€"):
            title = title[2:].strip()

        # ä¼šç¤¾å
        company_elem = card.find("span", {"data-testid": "company-name"})
        company = company_elem.get_text(strip=True) if company_elem else ""

        # å ´æ‰€
        location_elem = card.find("div", {"data-testid": "text-location"})
        if not location_elem:
            location_elem = card.find("div", {"data-testid": "icon-location"})
        location = location_elem.get_text(strip=True) if location_elem else ""

        # çµ¦ä¸ - salary-snippet-containerã‹ã‚‰å–å¾—
        salary_elem = card.find("li", class_="salary-snippet-container")

        wage_min = 0
        wage_max = 0
        wage_type = "unknown"
        employment_type = ""

        if salary_elem:
            salary_text = salary_elem.get_text(strip=True)

            # çµ¦ä¸ã‚¿ã‚¤ãƒ—åˆ¤å®š
            if "æ™‚çµ¦" in salary_text:
                wage_type = "hourly"
            elif "æœˆçµ¦" in salary_text or "æœˆå" in salary_text:
                wage_type = "monthly"
            elif "å¹´å" in salary_text or "å¹´ä¿¸" in salary_text:
                wage_type = "annual"

            # é‡‘é¡æŠ½å‡ºï¼ˆä¾‹: "æœˆçµ¦ 25ä¸‡å†† ~ 30ä¸‡å††" â†’ 250000, 300000ï¼‰
            # ã€Œä¸‡å††ã€ãƒ‘ã‚¿ãƒ¼ãƒ³
            man_pattern = re.findall(r"([\d.]+)\s*ä¸‡å††", salary_text)
            if man_pattern:
                wage_min = int(float(man_pattern[0]) * 10000)
                if len(man_pattern) > 1:
                    wage_max = int(float(man_pattern[1]) * 10000)
                else:
                    wage_max = wage_min
            else:
                # é€šå¸¸ã®æ•°å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¾‹: "1,800å††"ï¼‰
                yen_pattern = re.findall(r"([\d,]+)å††", salary_text.replace(",", ""))
                if yen_pattern:
                    wage_min = int(yen_pattern[0])
                    if len(yen_pattern) > 1:
                        wage_max = int(yen_pattern[1])
                    else:
                        wage_max = wage_min

            # å¹´åã‚’æœˆçµ¦ã«å¤‰æ›ï¼ˆ12ã§å‰²ã‚‹ï¼‰- çµ±ä¸€ã®ãŸã‚
            if wage_type == "annual" and wage_min > 0:
                wage_min = wage_min // 12
                wage_max = wage_max // 12
                wage_type = "monthly"  # æœˆçµ¦ã¨ã—ã¦çµ±ä¸€

        # é›‡ç”¨å½¢æ…‹ - çµ¦ä¸ä»¥å¤–ã®attribute_snippet_testidã‹ã‚‰å–å¾—
        attribute_elems = card.find_all(
            "li", {"data-testid": "attribute_snippet_testid"}
        )
        for attr in attribute_elems:
            if "salary-snippet-container" not in attr.get("class", []):
                text = attr.get_text(strip=True)
                if text in ["æ­£ç¤¾å“¡", "ã‚¢ãƒ«ãƒã‚¤ãƒˆãƒ»ãƒ‘ãƒ¼ãƒˆ", "æ´¾é£ç¤¾å“¡", "å¥‘ç´„ç¤¾å“¡"]:
                    employment_type = text
                    break

        # é‡‘é¡ã‹ã‚‰æ¨æ¸¬
        if wage_type == "unknown" and wage_min > 0:
            if wage_min < 10000:
                wage_type = "hourly"
            elif wage_min >= 100000:
                wage_type = "monthly"

        return {
            "title": title[:100] if title else "",
            "company": company,
            "location": location,
            "wage_min": wage_min,
            "wage_max": wage_max,
            "wage_type": wage_type,
            "employment_type": employment_type,
            "employment_type": employment_type,
            "source": "indeed",
            "url": url,
        }
    except Exception as e:
        print(f"  âš ï¸ ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def run_indeed_crawler(keyword="", location="æ±äº¬éƒ½", max_pages=3, headless=True):
    """
    Indeedã‹ã‚‰æ±‚äººã‚’åé›†
    undetected-chromedriverã§CAPTCHAå›é¿

    Args:
        keyword: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        location: åœ°åŸŸ
        max_pages: å–å¾—ãƒšãƒ¼ã‚¸æ•°
        headless: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰
    """
    print(f"ğŸ” Indeedæ¤œç´¢é–‹å§‹ (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword or 'å…¨ã¦'}, åœ°åŸŸ: {location})")

    # undetected_chromedriverã‚ªãƒ—ã‚·ãƒ§ãƒ³
    options = uc.ChromeOptions()
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")

    # undetected_chromedriverã§ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ä½œæˆï¼ˆCAPTCHAå›é¿æ©Ÿèƒ½å†…è”µï¼‰
    driver = uc.Chrome(options=options, headless=headless)
    wait = WebDriverWait(driver, 20)

    try:
        conn = get_connection()
        total_count = 0

        for page in range(max_pages):
            start = page * 10
            url = f"https://jp.indeed.com/jobs?q={keyword}&l={location}&start={start}"

            print(f"\nğŸ“¥ ãƒšãƒ¼ã‚¸ {page + 1}/{max_pages} ã‚’å–å¾—ä¸­...")
            driver.get(url)

            # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å®Œäº†ã‚’å¾…æ©Ÿ
            time.sleep(3)

            # ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’èª­ã¿è¾¼ã‚€
            driver.execute_script("window.scrollTo(0, 500);")
            time.sleep(1)
            driver.execute_script("window.scrollTo(0, 1000);")
            time.sleep(1)
            driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(1)

            # JavaScriptãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚’å¾…æ©Ÿ - æ±‚äººã‚«ãƒ¼ãƒ‰ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…ã¤
            try:
                wait.until(
                    EC.presence_of_element_located((By.CLASS_NAME, "job_seen_beacon"))
                )
            except:
                # åˆ¥ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™
                try:
                    wait.until(
                        EC.presence_of_element_located(
                            (By.CSS_SELECTOR, "[class*='cardOutline']")
                        )
                    )
                except:
                    print(f"  âš ï¸ æ±‚äººã‚«ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

            # æ±‚äººã‚«ãƒ¼ãƒ‰ã‚’å–å¾—
            html = driver.page_source
            soup = BeautifulSoup(html, "html.parser")

            # Indeedæ±‚äººã‚«ãƒ¼ãƒ‰ã‚»ãƒ¬ã‚¯ã‚¿
            job_cards = soup.find_all("div", class_="job_seen_beacon")
            if not job_cards:
                job_cards = soup.find_all("div", {"class": re.compile("cardOutline")})

            print(f"  ğŸ“‹ {len(job_cards)}ä»¶ã®æ±‚äººã‚’ç™ºè¦‹")

            page_count = 0
            skip_count = 0
            for card in job_cards:
                job_data = parse_indeed_job(card)
                if job_data and job_data["title"]:
                    # æ¥­ç•Œåˆ†é¡
                    job_data["industry"] = classify_industry(job_data["title"])

                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    try:
                        c = conn.cursor()
                        c.execute(
                            """
                            SELECT COUNT(*) FROM jobs 
                            WHERE title = ? AND company = ?
                            """,
                            (job_data["title"], job_data["company"]),
                        )
                        exists = c.fetchone()[0] > 0

                        if exists:
                            skip_count += 1
                            continue

                        # ä¿å­˜
                        c.execute(
                            """
                            INSERT INTO jobs 
                            (title, company, location, wage_min, wage_max, wage_type, industry, url)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                            """,
                            (
                                job_data["title"],
                                job_data["company"],
                                job_data["location"],
                                job_data["wage_min"],
                                job_data["wage_max"],
                                job_data["wage_type"],
                                job_data["industry"],
                                job_data.get("url", ""),
                            ),
                        )
                        page_count += 1
                    except Exception as e:
                        print(f"  âš ï¸ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

            conn.commit()
            total_count += page_count
            if skip_count > 0:
                print(f"  âœ… {page_count}ä»¶ã‚’ä¿å­˜ (é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {skip_count}ä»¶)")
            else:
                print(f"  âœ… {page_count}ä»¶ã‚’ä¿å­˜")

            time.sleep(2)

        print(f"\nğŸ‰ Indeedåé›†å®Œäº†ï¼ åˆè¨ˆ {total_count} ä»¶ã‚’ä¿å­˜")
        return {"success": True, "count": total_count}

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return {"success": False, "error": str(e)}
    finally:
        driver.quit()


if __name__ == "__main__":
    init_db()
    run_indeed_crawler(
        keyword="ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢", location="æ±äº¬éƒ½", max_pages=2, headless=False
    )
